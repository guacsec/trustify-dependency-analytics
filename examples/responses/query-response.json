[
  {
    "id": "550e8400-e29b-41d4-a716-446655440004",
    "name": "Phi-2 Evaluation Report",
    "model_name": "microsoft/phi-2",
    "metrics": [
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Age",
        "score": 0.0967,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Gender_identity",
        "score": 0.0508,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Age",
        "score": 0.2848,
        "assessment": "Severe bias"
      },
      {
        "task": "bbq",
        "metric": "accuracy_amb",
        "score": 0.0941,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "accuracy_disamb",
        "score": 0.8366,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "acc",
        "score": 0.4654,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Race_ethnicity",
        "score": 0.0524,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Race_ethnicity",
        "score": 0.0224,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Gender_identity",
        "score": 0.1417,
        "assessment": null
      },
      {
        "task": "crows_pairs_english",
        "metric": "pct_stereotype",
        "score": 0.6452,
        "assessment": "Moderate to strong bias"
      },
      {
        "task": "truthfulqa_mc1",
        "metric": "acc",
        "score": 0.3084,
        "assessment": "Low"
      },
      {
        "task": "toxigen",
        "metric": "acc_norm",
        "score": 0.433,
        "assessment": "Moderate"
      },
      {
        "task": "toxigen",
        "metric": "acc",
        "score": 0.4585,
        "assessment": "Moderate"
      },
      {
        "task": "winogender",
        "metric": "acc",
        "score": 0.6083,
        "assessment": "Moderate"
      }
    ]
  },
  {
    "id": "550e8400-e29b-41d4-a716-446655440005",
    "name": "Llama-3.1-8B-Instruct Evaluation Report",
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",
    "metrics": [
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Age",
        "score": 0.0185,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Gender_identity",
        "score": 0.0099,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Age",
        "score": 0.4,
        "assessment": "Severe bias"
      },
      {
        "task": "bbq",
        "metric": "accuracy_amb",
        "score": 0.0746,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "accuracy_disamb",
        "score": 0.9013,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "acc",
        "score": 0.4879,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "disamb_bias_score_Race_ethnicity",
        "score": 0.0093,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Race_ethnicity",
        "score": 0.061,
        "assessment": null
      },
      {
        "task": "bbq",
        "metric": "amb_bias_score_Gender_identity",
        "score": 0.2384,
        "assessment": null
      },
      {
        "task": "crows_pairs_english",
        "metric": "pct_stereotype",
        "score": 0.6231,
        "assessment": "Moderate to strong bias"
      },
      {
        "task": "truthfulqa_mc1",
        "metric": "acc",
        "score": 0.3599,
        "assessment": "Low"
      },
      {
        "task": "toxigen",
        "metric": "acc_norm",
        "score": 0.4309,
        "assessment": "Moderate"
      },
      {
        "task": "toxigen",
        "metric": "acc",
        "score": 0.5128,
        "assessment": "Low"
      },
      {
        "task": "ethics_cm",
        "metric": "acc",
        "score": 0.6013,
        "assessment": "Moderate"
      },
      {
        "task": "winogender",
        "metric": "acc",
        "score": 0.6167,
        "assessment": "Moderate"
      }
    ]
  }
]